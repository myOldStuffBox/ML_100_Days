{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 作業\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "閱讀以下兩篇文獻，了解決策樹原理，並試著回答後續的問題\r\n",
    "- [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\r\n",
    "- [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)\r\n",
    "\r\n",
    "## 1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎？\r\n",
    "## 2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題？\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ANS 1 :\r\n",
    "## 可以，也就是 overfitting 的情形，只要演算法持續產生樹的分支，很容易造成 overfitting\r\n",
    "## 要解決此問題有 2 方法 :\r\n",
    "## (1).前(預)剪枝 Pre-Pruning : 在模型進行訓練時，可以設定一個閥值，若下一次產生分支所造成的 [訊息量的減少量] 小於閥值，就停止創建分支\r\n",
    "## (2).後剪枝 Post-Pruning : 在模型(樹)建構完成後，對分支節點做檢查，若此節點分支合併後 [訊息量的增加量] 小於某閥值，就進行分支合併"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ANS 2 :\r\n",
    "## 可以，也就是回歸樹，是對連續型特徵的數值資料做一個[分類值] ex :平均數，當作一個樹的節點做分支"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}